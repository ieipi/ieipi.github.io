---
layout: article
title: 推荐算法实战-2-特征工程
tags: 2023 推荐系统 推荐算法实战
category: blog
pageview: true
mathjax: true
---
> 本文是推荐算法实战系列第二篇文章。

## 1、特征工程过时了吗 
在前深度学习时代，例如线性回归（LR）时代，特征工程是一项非常重要的工作，直接决定了模型的效果。

深度学习有一个理论，大意是说DNN可以作为“万能函数模拟器”，只要层数/参数足够多，DNN可以拟合输入与输出之间的任意函数关系。

深度学习出道时，例如AlexNet论文中，强调其“端到端学习”的特性，宣称深度学习可以自动学习特征表征，无需人工特征工程。

但是，在推荐系统中，特征工程过时了的观点是不正确的。

正如DCN（Deep Cross Network）论文指出的那样，DNN有时连简单的二阶、三阶特征交叉都模拟不好。在现实中，推荐系统数据集并不完美，所谓“万能模拟器”只是一种理论，梯度消失、梯度爆炸、不同特征受训机会不均衡等，都会影响DNN的性能。

个人觉得，特征工程代表着我们对数据分布的先验知识，模型结构是另一种先验知识，为了让DNN更好收敛到满足数据集分布要求的最优解，我们注入的先验知识越多越好。

## 2、特征提取 
特征域（field）和特征（feature）的区别是什么？

对于类别特征（例如“手机品牌”），通常需要通过one-hot encoding或者embedding编码成一个多维向量（1 * d）。

这个多维向量接入DNN模型前，通常需要和其它的特征向量（例如，“性别”）拼接成一个更高维的向量。

在高维向量中为了区分，将属于“手机品牌”的维度称为一个field，属于”性别“的维度称作另一个field。

推荐系统的特征维度具有高维稀疏的特点，通常有上亿维，但是field数其实没有那么多，数百个field差不多了。

特征提取的原则是不能重复或遗漏。可以从以下几个角度提取：物料画像、用户画像、交叉特征、偏差特征。

### 2.1、物料画像
#### 2.1.1、 物料自身属性
物料自身属性很多，我们可以自由发挥。不在此赘述。

其中最重要的是物料ID特征。ID特征，通常先要通过one-hot encoding编码成一个高维向量，再通过embedding得到一个低维稠密向量。

ID特征的特点是“强记忆性”和“弱泛化性”。一个ID特征唯一对应一个物料，在ID特征上学到的知识无法迁移到其它物料上。例如，模型学习到用户喜欢“ID001”的物品，但是换一个“ID002”的物品，模型无法泛化。因为“ID002”和“ID001”是完全独立的两个id。

是否应该引入ID类特征？

首先，考虑id特征在数据中出现的频率，判断id特征是否能充分学习。推荐系统通常具有马太效应，无论是用户ID还是物料ID在数据集中出现的频率都呈现幂律分布（长尾分布）。即少量的用户、物料占据了样本中的绝大多数，它们的embedding能够充分的学习；其余大量尾部的ID在样本中出现频率低，其embedding在训练时没经过几次参数更新，学习不充分，与随机初始化好不了多少。

其次，考虑ID特征添加在模型结构的什么位置。ID特征可以和其他特征简单拼接在一起吗？

物品池大小适中，且基本保持稳定的场景，item_id可以和其它特征一起训练。

userid等强区分性特征可以放在单独的塔中学习user bias，不和其它特征一起。

在长尾分布严重的场景，userid、itemid等强区分性特征embedding可以做特征粒度的dropout后再与其它特征拼接。这里的dropout是指embedding整体有一定概率被丢弃（mask）。

记忆型特征单独放在一个塔中，泛化型特征放在另外的塔中。引入一个基于物品分布的门控机制，让头部的物品主要拟合“记忆特征”，中尾部的物品主要拟合“泛化特征”。通过加权求和的方式融合各个特征。参考google的cross decoupling network（DCN）。

#### 2.1.2、 物料的类别与标签
主要是物料的静态画像，即不依赖用户反馈的物料信息，包括物料的类别、标签等。

如果物料包含文本、图像等内容，还可以用NLP/CV算法预处理，提取标签。

#### 2.1.3、 物料的基于内容的embedding
静态标签的优点是可解释，缺点是稀疏。为此，可以利用CNN/BERT等深度模型提取内容的embedding信息，作为特征。

#### 2.1.4、 物料的动态画像
所谓动态画像指基于用户行为反馈的后验统计数据。

#### 2.1.5、 用户给物料打标
通常是把物料的标签聚合到用户身上，丰富用户画像。

也可以反过来，将消费过该物料的用户身上的标签，聚合到物料上，丰富物料画像。

### 2.2、用户画像
#### 2.2.1、用户的静态画像
主要指人口属性信息。

值得考虑的是，新老用户一起建模时，老用户的样本多，主导了训练过程。而老用户的行为信息丰富。结果是，模型可能不会重视新用户友好的静态画像特征。

#### 2.2.2、用户的动态画像
主要是从用户的历史行为中提取的兴趣爱好。

#### 2.3、交叉特征
交叉特征又称组合特征，通过组合特征，可以帮助识别样本中特定的模式。例如，将“国家”、“时间”、“物品类别”三个特征交叉，得到新的特征“国家 * 时间 * 物品类别”，可以识别“美国 * 圣诞节 * 圣诞礼物”的样本点击率很高。

### 2.4、偏差特征
比较常见的bias特征有position bias和视频年龄bias。

position bias的意思是，不同position的固有点击概率不一样。用户感兴趣的物品，如果曝光位置比较靠后，也可能没有点击。

视频年龄bias是youtube论文中提出来的。上传早的视频，有足够长的时间积累人气，后验指标更好，模型排名更高。为了减轻这一偏差，Youtube在训练时将视频年龄作为偏差特征喂入模型，在预测时统一设置为0。

position bias也可以用同样的方式处理。

这种预测时拿不到值的特征，称为dummy feature。为了避免dummy feature设置不同的值影响预测时物品的排序结果，只能将dummy feature通过一个线性层，和deep网络在logits层相加。不能和其它特征拼接一起喂入DNN。即：

$$logit = DNN(u,t) + w^Tb$$

这是因为，DNN对特征做非线性交叉，预测时dummy feature取不同的值会产生不同的排序结果。

## 3、数值特征的处理 
### 3.1、缺失值的处理
常见方法有：用样本均值等代替、模型预测缺失值补充等。

### 3.2、标准化
有些特征是长尾分布，直接用z-score标准化会被长尾数据带偏。

一种处理方式是，先对原始数据开方、取对数等非线性变换，将原始数据变成接近正态分布，再做z-score变换。

### 3.3、平滑与消偏
有些比例类特征，由于样本稀疏，导致结果不置信。例如，一件商品只曝光了一次，并被购买了，我们不能说它的购买率是100%。

为了克服样本稀疏问题，提高置信度，可以采用“威尔逊区间平滑”。

### 3.4、分桶离散化
数值特征分桶离散化后变成了类别特征，更容易刻画非线性关系。

## 4、类别特征的处理 
类别特征是推荐算法的一等公民，享受VIP待遇。类别特征的一大特点是高维、稀疏。

类别特征更容易表达输入和目标之间的非线性关系。将类别特征embedding化后，能扩充其内涵，具有“隐语义”。例如，“用户年龄在20-30之间”，embedding化之后，可以反映用户“经济实力有限、审美风格时尚”等丰富的含义，这是单一的数值特征远远不能比的。

高维稀疏的类别特征，缺少训练机会，可能导致模型训练不充分。

例如，对于和的交叉特征前面的权重，受训的机会很少。LR只拿和都不为0的样本训练。FM缓解了这个问题，只要和有一个不为0的样本，都可以参与训练。

类别特征是字符串，不能直接喂入模型处理。需要先数字化。通常分两步：

第一步，通过one-hot encoding或者hash，得到0-N之间的整数，其中N为embedding 矩阵的行数。

第二步，查表。embedding矩阵第i行的向量，就是当前类别值的embedding特征。

embedding矩阵，通常作为模型参数，和上层网络一起训练。事实上，深度模型的参数大部分都是embedding矩阵的参数。

---


<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_page_pv">本文总访问量<span id="busuanzi_value_page_pv"></span>次</span>